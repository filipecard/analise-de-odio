{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustar tamanho máximo de linha visível. Max caracteres por tweet: 280\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar dados do dataset encoding incompatível com utf-8\n",
    "data = pd.read_csv(\"train.csv\", encoding='ISO-8859-1')\n",
    "#Remover colunas desncessárias\n",
    "data = data.drop(columns=['ItemID','Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar coluna com o tamanho de caracteres de cada tweet\n",
    "data['length'] = [len(tweet[0]) for tweet in data.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linhas com mais e menos caracteres\n",
    "data.sort_values(by='length', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados sobre a quantidade de caracteres\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de emoticons\n",
    "fonte: https://www.kaggle.com/youben/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contagem (com alterações)\n",
    "tweets_text = data.SentimentText.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-']?\\S) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "sorted(emos_count,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Busca (com alterações)\n",
    "HAPPY_EMO = r\" ([xX;:]-?[bdD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[xXsS/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palavras mais usadas\n",
    "fonte: https://www.kaggle.com/youben/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used_words(data.SentimentText.str.cat())[:100] #100 mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mais frequentes sem stopwords\n",
    "mw = most_used_words(data.SentimentText.str.cat())\n",
    "most_words = []\n",
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "    if w in stopwords.words(\"english\"):\n",
    "        continue\n",
    "    else:\n",
    "        most_words.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpeza de texto 01\n",
    "def clean_text_round1(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'@\\w+\\s', ' ', text) #Remover nomes de usuários\n",
    "    text = re.sub(r'https?://.+\\S', ' ', text) #Remover links\n",
    "    text = re.sub('&[\\w*%s*]' % re.escape(string.punctuation), ' ', text) #Apagar '&' e caracteres posteriores \n",
    "    text = re.sub(r'#', ' ', text) #Remove hashtags\n",
    "    text = re.sub('\\s{2,}', '', text) #Remover excesso de espaços\n",
    "    text = re.sub(r'\\s', ' ', text) #Remover whitespaces    \n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar novo dataframe\n",
    "clean = pd.DataFrame.from_dict(data.SentimentText.apply(round1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar coluna com o tamanho de caracteres de cada texto\n",
    "clean['length'] = [len(text[0]) for text in clean.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.describe()\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpeza de texto 02\n",
    "def clean_text_round2(text):\n",
    "    #text = re.sub(r'\\W', \" \", text)\n",
    "    text = re.sub(r'(.)\\1{1,}', r'\\1', text) #Remove repetições MELHORAR\n",
    "    return text\n",
    "\n",
    "round2 = lambda x : clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.SentimentText = pd.DataFrame.from_dict(clean.SentimentText.apply(round2))\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover linhas com menos de 2 caracteres ou mais de 300\n",
    "new = np.array([[line[0], int(line[1])] for line in clean.values if 300 > line[1] > 2])\n",
    "new = pd.DataFrame.from_dict(new)\n",
    "new.columns = ['text', 'length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_int = lambda x: int(x)\n",
    "new['length'] = new.length.apply(to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.sort_values(by='length', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(new.text.head(10000))\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = new.head(10000).index\n",
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
