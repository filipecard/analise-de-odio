{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ajustar tamanho máximo de linha visível. Max caracteres por tweet: 280\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar dados do dataset encoding incompatível com utf-8\n",
    "data = pd.read_csv(\"train.csv\", encoding='ISO-8859-1')\n",
    "#Remover colunas desncessárias\n",
    "data = data.drop(columns=['ItemID','Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar coluna com o tamanho de caracteres de cada tweet\n",
    "data['length'] = [len(tweet[0]) for tweet in data.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8834</td>\n",
       "      <td>brokeback mountain was terrible.\\n8838,0,Sentiment140,# @Catherine42 I wouldn't mind but I only had 1/2 a portion &amp;amp; then left 1/2 the cream  just fruit for me then until my hols  x\\n8839,1,Sentiment140,# @DeliciousLunch ... dark chocolate cookies? oh you tease! I'm writing to day n dipping ...</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11346</td>\n",
       "      <td>#self_info 2ë¬ë? ê°ë°ì?... ì?¸ê°ì ?ì?¸ ê°ë°ì?ë¥¼ ê¿ê¿ëë¤. íì¬ ì§ì­ê´ê³ ìë²ë¥¼ íê³ , ê°ì?¸ì ?ì¼ë¡ë ê²ììì§ì? ë§ë¤ë ¤ê³  ì¤ë¹ì¤ì",
       "ëë¤. ì¤ë¹. . .ã",
       "ã",
       "ã",
       " íë¡ì° ê³ ê³ ê³ ~   ê¸°ì ìì? ì¬ëìì§ ì¬ëìì? ê¸°ì ìë¤~!</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33208</td>\n",
       "      <td>@ahohey  project design à¹à¸à¹à¸?à¸à¸µà¹à¸à¸µà¸à¸µà¹à¸¡à¸«à¹à¸­à¸­à¸?à¹?à¸à¸à¹à¸£à¸à¸à¸¢à¸²à¸à¸²à¸¥....à¸?à¸³à¸¥à¸±à¸à¸à¸³à¹à¸à¸£à¹?à¸?à¸£à¸¡à¸­à¸¢à¸¹à¹....à¹à¸à¸´à¸?à¹à¸à¸¢à¸¡à¸² comment à¸à¸µà¸?à¸§à¹à¸² 5555    function à¹à¸¢à</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9495</td>\n",
       "      <td>&amp;quot;Ð¯ Ð¿Ð¾Ð´Ð¿Ð¸Ñ?Ð°Ð»Ñ?Ñ? Ð½Ð° Ð²Ð°ÑÑ RSS Ð»ÐµÐ½ÑÑ, Ð½Ð¾ Ñ?Ð¾Ð¾Ð±ÑÐµÐ½Ð¸Ñ? Ð¿Ð¾ÑÐµÐ¼Ñ-ÑÐ¾ Ð² Ð²Ð¸Ð´Ðµ ÐºÐ°ÐºÐ¸Ñ",
       "-ÑÐ¾ ÐºÐ²Ð°Ð´ÑÐ°ÑÐ¾Ð²  ÐÐ°Ðº Ñ?ÑÐ¾ Ð¸Ñ?Ð¿ÑÐ°Ð²Ð¸ÑÑ?&amp;quot; ÐÐ¾Ð¹ÑÐ¸ Ð½Ð°Ñ",
       " Ð¸ Ð·Ð°Ð±ÑÑÑ Ð¿ÑÐ¾ Ð¼Ð¾Ñ RSS ))</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46373</td>\n",
       "      <td>@ApartHotel ÑÑ Ñ?ÐºÐ¾ÑÐµÐµ Ð¿Ð¾ÑÐµÑÑ?ÐµÑÑ Ð¸Ð»Ð¸ Ð¿Ð¾Ð¼ÐµÐ½Ñ?ÐµÑÑ ÑÐµÐ»ÐµÑÐ¾Ð½ Ð¿ÑÐµÐ¶Ð´Ðµ ÑÐµÐ¼ Ñ?Ð´Ð¾Ñ",
       "Ð½ÐµÑ Ð°ÐºÐºÑÐ¼ÑÐ»Ñ?ÑÐ¾Ñ  ÐÐ°ÑÐ¾ Ð¿ÑÐ¾ÑÐ½Ð¾Ñ?ÑÑ Ñ?Ð±Ð¾ÑÐºÐ¸ Ð±ÐµÑ?Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð°Ñ? - ÐºÐ°Ðº Ð¼Ð¾Ð½Ð¾Ð»Ð¸Ñ!</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34061</td>\n",
       "      <td>@ajbee</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12256</td>\n",
       "      <td>*heart</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4192</td>\n",
       "      <td>yeesh</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2014</td>\n",
       "      <td>FOOD!</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3776</td>\n",
       "      <td>just</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                     SentimentText  \\\n",
       "8834    brokeback mountain was terrible.\\n8838,0,Sentiment140,# @Catherine42 I wouldn't mind but I only had 1/2 a portion &amp; then left 1/2 the cream  just fruit for me then until my hols  x\\n8839,1,Sentiment140,# @DeliciousLunch ... dark chocolate cookies? oh you tease! I'm writing to day n dipping ...   \n",
       "11346                           #self_info 2ë¬ë? ê°ë°ì?... ì?¸ê°ì ?ì?¸ ê°ë°ì?ë¥¼ ê¿ê¿ëë¤. íì¬ ì§ì­ê´ê³ ìë²ë¥¼ íê³ , ê°ì?¸ì ?ì¼ë¡ë ê²ììì§ì? ë§ë¤ë ¤ê³  ì¤ë¹ì¤ì\n",
       "ëë¤. ì¤ë¹. . .ã\n",
       "ã\n",
       "ã\n",
       " íë¡ì° ê³ ê³ ê³ ~   ê¸°ì ìì? ì¬ëìì§ ì¬ëìì? ê¸°ì ìë¤~!   \n",
       "33208                                              @ahohey  project design à¹à¸à¹à¸?à¸à¸µà¹à¸à¸µà¸à¸µà¹à¸¡à¸«à¹à¸­à¸­à¸?à¹?à¸à¸à¹à¸£à¸à¸à¸¢à¸²à¸à¸²à¸¥....à¸?à¸³à¸¥à¸±à¸à¸à¸³à¹à¸à¸£à¹?à¸?à¸£à¸¡à¸­à¸¢à¸¹à¹....à¹à¸à¸´à¸?à¹à¸à¸¢à¸¡à¸² comment à¸à¸µà¸?à¸§à¹à¸² 5555    function à¹à¸¢à   \n",
       "9495                                                          &quot;Ð¯ Ð¿Ð¾Ð´Ð¿Ð¸Ñ?Ð°Ð»Ñ?Ñ? Ð½Ð° Ð²Ð°ÑÑ RSS Ð»ÐµÐ½ÑÑ, Ð½Ð¾ Ñ?Ð¾Ð¾Ð±ÑÐµÐ½Ð¸Ñ? Ð¿Ð¾ÑÐµÐ¼Ñ-ÑÐ¾ Ð² Ð²Ð¸Ð´Ðµ ÐºÐ°ÐºÐ¸Ñ\n",
       "-ÑÐ¾ ÐºÐ²Ð°Ð´ÑÐ°ÑÐ¾Ð²  ÐÐ°Ðº Ñ?ÑÐ¾ Ð¸Ñ?Ð¿ÑÐ°Ð²Ð¸ÑÑ?&quot; ÐÐ¾Ð¹ÑÐ¸ Ð½Ð°Ñ\n",
       " Ð¸ Ð·Ð°Ð±ÑÑÑ Ð¿ÑÐ¾ Ð¼Ð¾Ñ RSS ))   \n",
       "46373                                                                @ApartHotel ÑÑ Ñ?ÐºÐ¾ÑÐµÐµ Ð¿Ð¾ÑÐµÑÑ?ÐµÑÑ Ð¸Ð»Ð¸ Ð¿Ð¾Ð¼ÐµÐ½Ñ?ÐµÑÑ ÑÐµÐ»ÐµÑÐ¾Ð½ Ð¿ÑÐµÐ¶Ð´Ðµ ÑÐµÐ¼ Ñ?Ð´Ð¾Ñ\n",
       "Ð½ÐµÑ Ð°ÐºÐºÑÐ¼ÑÐ»Ñ?ÑÐ¾Ñ  ÐÐ°ÑÐ¾ Ð¿ÑÐ¾ÑÐ½Ð¾Ñ?ÑÑ Ñ?Ð±Ð¾ÑÐºÐ¸ Ð±ÐµÑ?Ð¿Ð¾Ð´Ð¾Ð±Ð½Ð°Ñ? - ÐºÐ°Ðº Ð¼Ð¾Ð½Ð¾Ð»Ð¸Ñ!   \n",
       "...                                                                                                                                                                                                                                                                                                            ...   \n",
       "34061                                                                                                                                                                                                                                                                                                      @ajbee    \n",
       "12256                                                                                                                                                                                                                                                                                                      *heart    \n",
       "4192                                                                                                                                                                                                                                                                                                        yeesh    \n",
       "2014                                                                                                                                                                                                                                                                                                        FOOD!    \n",
       "3776                                                                                                                                                                                                                                                                                                         just    \n",
       "\n",
       "       length  \n",
       "8834      949  \n",
       "11346     274  \n",
       "33208     255  \n",
       "9495      244  \n",
       "46373     237  \n",
       "...       ...  \n",
       "34061       7  \n",
       "12256       7  \n",
       "4192        7  \n",
       "2014        7  \n",
       "3776        6  \n",
       "\n",
       "[99989 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linhas com mais e menos caracteres\n",
    "data.sort_values(by='length', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados sobre a quantidade de caracteres\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de emoticons\n",
    "fonte: https://www.kaggle.com/youben/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3281, ':/'),\n",
       " (1339, 'x@'),\n",
       " (1214, 'xx'),\n",
       " (1162, 'xa'),\n",
       " (984, ';3'),\n",
       " (887, 'xp'),\n",
       " (842, 'xo'),\n",
       " (713, ';)'),\n",
       " (483, 'xe'),\n",
       " (431, ';I'),\n",
       " (353, ';.'),\n",
       " (254, 'xD'),\n",
       " (251, 'x.'),\n",
       " (245, '::'),\n",
       " (217, ';t'),\n",
       " (209, ';s'),\n",
       " (185, ':O'),\n",
       " (176, ':3'),\n",
       " (166, ';D'),\n",
       " (159, \":'\"),\n",
       " (157, 'XD'),\n",
       " (146, 'x3'),\n",
       " (142, ':p'),\n",
       " (126, \":'(\"),\n",
       " (118, ':@'),\n",
       " (117, 'xh'),\n",
       " (117, ':S'),\n",
       " (109, 'xm'),\n",
       " (104, ';p'),\n",
       " (104, ';-)'),\n",
       " (92, ':|'),\n",
       " (91, 'x,'),\n",
       " (89, ';P'),\n",
       " (76, 'xd'),\n",
       " (75, ';o'),\n",
       " (75, ';d'),\n",
       " (71, ':o'),\n",
       " (65, 'XX'),\n",
       " (63, ':L'),\n",
       " (59, 'Xx'),\n",
       " (59, ':1'),\n",
       " (58, ':]'),\n",
       " (57, ':s'),\n",
       " (56, ':0'),\n",
       " (54, 'XO'),\n",
       " (44, ';;'),\n",
       " (43, ';('),\n",
       " (38, ':-D'),\n",
       " (37, 'xk'),\n",
       " (36, 'XT'),\n",
       " (35, 'x?'),\n",
       " (35, 'x)'),\n",
       " (34, 'x2'),\n",
       " (33, ';/'),\n",
       " (32, 'x:'),\n",
       " (32, ':\\\\'),\n",
       " (31, 'x-'),\n",
       " (27, 'Xo'),\n",
       " (27, 'XP'),\n",
       " (27, ':-/'),\n",
       " (26, ':-P'),\n",
       " (25, ':*'),\n",
       " (23, 'xX'),\n",
       " (22, \":')\"),\n",
       " (17, 'xP'),\n",
       " (16, ':['),\n",
       " (16, ':-p'),\n",
       " (14, 'x]'),\n",
       " (14, 'XM'),\n",
       " (13, ':-O'),\n",
       " (12, 'x('),\n",
       " (12, 'X1'),\n",
       " (12, ':x'),\n",
       " (11, 'XS'),\n",
       " (11, ':l'),\n",
       " (10, 'x*'),\n",
       " (10, 'X.'),\n",
       " (10, ':b'),\n",
       " (10, ':T'),\n",
       " (9, ';]'),\n",
       " (9, ':I'),\n",
       " (8, ':C'),\n",
       " (7, ';-('),\n",
       " (7, ':-|'),\n",
       " (6, 'X,'),\n",
       " (6, ':-o'),\n",
       " (6, ':-\\\\'),\n",
       " (6, ':-*'),\n",
       " (6, ':$'),\n",
       " (5, 'XL'),\n",
       " (5, ':d'),\n",
       " (5, ':X'),\n",
       " (5, ':H'),\n",
       " (5, ':?'),\n",
       " (5, ':-S'),\n",
       " (4, ';-D'),\n",
       " (3, ':Z'),\n",
       " (3, ':E'),\n",
       " (3, ':-s'),\n",
       " (3, ':-['),\n",
       " (3, ':-X'),\n",
       " (2, 'X5'),\n",
       " (2, 'X-('),\n",
       " (2, \"X's\"),\n",
       " (2, ';-;'),\n",
       " (2, ':}'),\n",
       " (2, ':D'),\n",
       " (2, ':;'),\n",
       " (2, \":'D\"),\n",
       " (1, 'x|'),\n",
       " (1, \"x'd\"),\n",
       " (1, \"x'D\"),\n",
       " (1, ';-|'),\n",
       " (1, ';-/'),\n",
       " (1, ':-x'),\n",
       " (1, ':-h'),\n",
       " (1, ':-]'),\n",
       " (1, ':-W'),\n",
       " (1, ':-$'),\n",
       " (1, ':('),\n",
       " (1, \":'[\")]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Contagem (com alterações)\n",
    "tweets_text = data.SentimentText.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-']?\\S) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "sorted(emos_count,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Busca (com alterações)\n",
    "HAPPY_EMO = r\" ([xX;:]-?[bdD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[xXsS/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palavras mais usadas\n",
    "fonte: https://www.kaggle.com/youben/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used_words(data.SentimentText.str.cat())[:100] #100 mais frequentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mais frequentes sem stopwords\n",
    "mw = most_used_words(data.SentimentText.str.cat())\n",
    "most_words = []\n",
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "    if w in stopwords.words(\"english\"):\n",
    "        continue\n",
    "    else:\n",
    "        most_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['box',\n",
       " 'radio',\n",
       " 'kidding',\n",
       " 'iphone',\n",
       " 'air',\n",
       " 'minute',\n",
       " 'warm',\n",
       " 'power',\n",
       " 'country',\n",
       " 'test',\n",
       " 'flight',\n",
       " 'YES',\n",
       " 'OK',\n",
       " 'Chris',\n",
       " 'LMAO']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_words[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = stem_tokenize(\" \".join(most_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['box',\n",
       " 'radio',\n",
       " 'kidding',\n",
       " 'iphone',\n",
       " 'air',\n",
       " 'minute',\n",
       " 'warm',\n",
       " 'power',\n",
       " 'country',\n",
       " 'test',\n",
       " 'flight',\n",
       " 'YES',\n",
       " 'OK',\n",
       " 'Chris',\n",
       " 'LMAO']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparação do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpeza de texto 01\n",
    "def clean_text_round1(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r'@\\w+\\s?', ' ', text) #Remover nomes de usuários\n",
    "    text = re.sub(r'https?://.+\\S', ' ', text) #Remover links\n",
    "    text = re.sub('&[\\w*%s*]' % re.escape(string.punctuation), ' ', text) #Apagar '&' e caracteres posteriores \n",
    "    text = re.sub(r'#', ' ', text) #Remove hashtags\n",
    "    text = re.sub('\\s{2,}', '', text) #Remover excesso de espaços\n",
    "    text = re.sub(r'\\s', ' ', text) #Remover whitespaces    \n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar novo dataframe\n",
    "clean = pd.DataFrame.from_dict(data.SentimentText.apply(round1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar coluna com o tamanho de caracteres de cada texto\n",
    "clean['length'] = [len(text[0]) for text in clean.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar arquivo\n",
    "clean.to_csv('clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend.............</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>i missed the new moon trailer...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>omg its already 7:30 :o</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>.. omgaga. im soooim gunna cry. i've been at this dentist since 11.. i was suposed 2 just get a crown put on (30mins)...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>i think mi bf is cheating on me!!!t_t</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99984</td>\n",
       "      <td>seems like a repeating problemhope you're able to find something.</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99985</td>\n",
       "      <td>arrrr we both replied to each other over different tweets at the same time, i'll see you then, duno where the hell kateyy is!</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99986</td>\n",
       "      <td>ya i thought so</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99987</td>\n",
       "      <td>yes. yes. i'm glad you had more fun with me.</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99988</td>\n",
       "      <td>haha yes you do</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        SentimentText  \\\n",
       "0                                                                                            is so sad for my apl friend.............   \n",
       "1                                                                                                    i missed the new moon trailer...   \n",
       "2                                                                                                             omg its already 7:30 :o   \n",
       "3            .. omgaga. im soooim gunna cry. i've been at this dentist since 11.. i was suposed 2 just get a crown put on (30mins)...   \n",
       "4                                                                                               i think mi bf is cheating on me!!!t_t   \n",
       "...                                                                                                                               ...   \n",
       "99984                                                               seems like a repeating problemhope you're able to find something.   \n",
       "99985   arrrr we both replied to each other over different tweets at the same time, i'll see you then, duno where the hell kateyy is!   \n",
       "99986                                                                                                                 ya i thought so   \n",
       "99987                                                                                    yes. yes. i'm glad you had more fun with me.   \n",
       "99988                                                                                                                 haha yes you do   \n",
       "\n",
       "       length  \n",
       "0          40  \n",
       "1          32  \n",
       "2          23  \n",
       "3         120  \n",
       "4          37  \n",
       "...       ...  \n",
       "99984      65  \n",
       "99985     126  \n",
       "99986      16  \n",
       "99987      45  \n",
       "99988      16  \n",
       "\n",
       "[99989 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.describe()\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpeza de texto 02\n",
    "def clean_text_round2(text):\n",
    "    #text = re.sub(r'\\W', \" \", text)\n",
    "    text = re.sub(r'(.)\\1{3,}', r'\\1', text) #Remove repetições MELHORAR\n",
    "    return text\n",
    "\n",
    "round2 = lambda x : clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend.</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>i missed the new moon trailer...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>omg its already 7:30 :o</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>.. omgaga. im soooim gunna cry. i've been at this dentist since 11.. i was suposed 2 just get a crown put on (30mins)...</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>i think mi bf is cheating on me!!!t_t</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99984</td>\n",
       "      <td>seems like a repeating problemhope you're able to find something.</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99985</td>\n",
       "      <td>ar we both replied to each other over different tweets at the same time, i'll see you then, duno where the hell kateyy is!</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99986</td>\n",
       "      <td>ya i thought so</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99987</td>\n",
       "      <td>yes. yes. i'm glad you had more fun with me.</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99988</td>\n",
       "      <td>haha yes you do</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                     SentimentText  \\\n",
       "0                                                                                                     is so sad for my apl friend.   \n",
       "1                                                                                                 i missed the new moon trailer...   \n",
       "2                                                                                                          omg its already 7:30 :o   \n",
       "3         .. omgaga. im soooim gunna cry. i've been at this dentist since 11.. i was suposed 2 just get a crown put on (30mins)...   \n",
       "4                                                                                            i think mi bf is cheating on me!!!t_t   \n",
       "...                                                                                                                            ...   \n",
       "99984                                                            seems like a repeating problemhope you're able to find something.   \n",
       "99985   ar we both replied to each other over different tweets at the same time, i'll see you then, duno where the hell kateyy is!   \n",
       "99986                                                                                                              ya i thought so   \n",
       "99987                                                                                 yes. yes. i'm glad you had more fun with me.   \n",
       "99988                                                                                                              haha yes you do   \n",
       "\n",
       "       length  \n",
       "0          40  \n",
       "1          32  \n",
       "2          23  \n",
       "3         120  \n",
       "4          37  \n",
       "...       ...  \n",
       "99984      65  \n",
       "99985     126  \n",
       "99986      16  \n",
       "99987      45  \n",
       "99988      16  \n",
       "\n",
       "[99989 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean.SentimentText = pd.DataFrame.from_dict(clean.SentimentText.apply(round2))\n",
    "clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remover linhas com menos de 2 caracteres ou mais de 300\n",
    "new = np.array([[line[0], int(line[1])] for line in clean.values if 300 > line[1] > 2])\n",
    "new = pd.DataFrame.from_dict(new)\n",
    "new.columns = ['text', 'length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_int = lambda x: int(x)\n",
    "new['length'] = new.length.apply(to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvar data\n",
    "new.to_csv(\"new_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.sort_values(by='length', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(new.text.head(10000))\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = new.head(10000).index\n",
    "data_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
