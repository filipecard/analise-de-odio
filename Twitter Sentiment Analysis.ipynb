{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento de texto a partir do dataset:\n",
    "\n",
    "## > School of AI Algiers Challenge - Twitter Sentiment Analysis\n",
    "\n",
    "fonte: https://www.kaggle.com/youben/twitter-sentiment-analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This is for making some large tweets to be displayed\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# I got some encoding issue, I didn't knew which one to use !\n",
    "# This post suggested an encoding that worked!\n",
    "# https://stackoverflow.com/questions/19699367/unicodedecodeerror-utf-8-codec-cant-decode-byte\n",
    "train_data = pd.read_csv(\"train.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56280                                          @BaltarStar Wow! No cliche left unturned (and fluffy hair!). \n",
       "88117            @aydeejay awwwwww.... YOU CAN CLEAN THE GLASS SLiPPERS CiNDERELLA, BUT YOU CANT HAVE THEM. \n",
       "99986                                                                         @CuPcAkE_2120 ya i thought so \n",
       "557                 - thelovelybones: I plan on owning this â¦donât judge me. http://tumblr.com/xr0238tmq\n",
       "65412                          @BlackBoxBelfast it's not Sunday - at least you can wangle a cup of Clements \n",
       "7762                                                                          #myweakness F0OD iN GENERAL.. \n",
       "48084    @arnoldwender Cool! And I hope you won't get any. I lost my HD but luckily I saved my profile in...\n",
       "58733                  @Becky_x_x_ I don't have a fave three, just diversity  but sean smith is gorgeous too\n",
       "43998    @Anne_Frasier ha! have some of our rain. been raining so much can't drive my new jeep  i just wa...\n",
       "38734      @amurodesu æä»å¤©æ²çå°ä½ ...that was probs poor grammar. sigh sad sad me...oh man  dramzzz\n",
       "71968                               @bridgethaddix  a cool washcloth on ur forehead usually helps a lil bit!\n",
       "73689    @anna_minx i dont make excuses. Just letting you know i wont beat you by as much i would like  j...\n",
       "9662                                                   &quot;Koogle&quot;... They should have called Joogle \n",
       "97682                                                @ColdZero2006 Nice! Give me a shout if you want a hand \n",
       "88531                                                      @CindyRae2 First Methodist to speak to the youth \n",
       "72791    @angieODT girl! life is good! i'm so excited about everyone going to shows! and everyone meeting...\n",
       "55926    @backstreetboys Hi boys!! How are u?? Hey my birthday is in June 4th!!  I love you so much guys!...\n",
       "97790                                          @ColleenKRich: Never, never, never give up! Thanks 4 the FF! \n",
       "4313                                                                                not a good day at uni...\n",
       "98531    @CorinaBecker haha yeah sorry about that ... Did think that part LOL at least you have experienc...\n",
       "82274                   @ashleymajher... I not lol where is you?? I don't like waking up with u not next me \n",
       "24695    @24_7_Hustle_Ent It really is..it's just updating your friends, fans ect on what you're doing an...\n",
       "52080                                    @AshlieRayann Ashlie, thank you.  You made me feel a little better.\n",
       "81307                                                                              @chasingangel82 oh no bb \n",
       "37990                                           @allisonweiss duuude  *huggles* haters gonna hate, they suck\n",
       "17500    @_algin_  lol Catholic falls under Christian sweetheart. SO I guess that makes me both. Sorry to...\n",
       "26549                                                                             @9teen8ty9 i like to run! \n",
       "89697                                                           @clarklee Crystal and I need your services. \n",
       "54366                               @avinashmeetoo I'm just reminiscing my stay at the hospital, last year. \n",
       "59318    @berriesweetest Sounds interesting. Yeah, my brothers &amp; I don't know spanish but my mom &amp...\n",
       "13201    &quot;I think, therefore I'm dangerous.&quot;  A new twist to the old maxim. I find I like it be...\n",
       "36813                                                    @alexxyy cause i go every friday morning  i have to\n",
       "39091    @alwaysloveu_Ci Well, I guess it would &amp; I don't blame her but as long as we got her back, s...\n",
       "33834                                                                                       @allegra0 what? \n",
       "61928                                 @binmugahid hey! maybe they are creating their own Google street view \n",
       "45319                                                                         @aliciaway I'd miss Mikey too \n",
       "81312    @chasingtuesday Ugh you're lucky! I don't think I have channel 110! Do you have Satellite? Cause...\n",
       "33217    @ahrenfelt  have a chance to see Christiania or Stroget? What about eating those amazing chocola...\n",
       "86096      @chrisoakley Keep the Kodak Carousel in your trousers chum  Are you coming down the pub tomorrow?\n",
       "13563                                                                             ... 2 more days of work!! \n",
       "7559     #location Â» Relocated from the Red Sea coast to Giza, in the apt of a friend with great view of...\n",
       "69240    @bucky4eyes did I miss your birthday? is it today or yesterday? Happy Birthday if today, Belated...\n",
       "4629                                               #brum09 afternoon tea - cookies are a bit stale  #ri09con\n",
       "96754                    @Clostar14 don't be nervousss &amp; Its playing dr pressure right now ahah love it \n",
       "44379    @anniea89 Sanity @ Randwick closed about 5 months ago! :'( This saddens me deeply as I'm in walk...\n",
       "6150       #iphone seems nuevasync is no longer working   But Google Sync is.... thanks for the help google!\n",
       "90384     @cloudsteph 'Fallon &amp; Byrne'... sounds like something happened to me on Bonfire Night once... \n",
       "32374             @alibrarian you should be ;) it was a great Reflections show ;) plus it's not raining now \n",
       "91670    @coldplay http://twitpic.com/7oigi - wow! good luck! I would like to be there  will you come to ...\n",
       "53246                                                                                       @B_Real420 Why? \n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will now take a look at random tweets\n",
    "# to gain more insights\n",
    "\n",
    "rand_indexs = np.random.randint(1,len(train_data),50).tolist()\n",
    "train_data[\"SentimentText\"][rand_indexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3281, ':/'),\n",
       " (2874, 'x '),\n",
       " (2626, ': '),\n",
       " (1339, 'x@'),\n",
       " (1214, 'xx'),\n",
       " (1162, 'xa'),\n",
       " (984, ';3'),\n",
       " (887, 'xp'),\n",
       " (842, 'xo'),\n",
       " (713, ';)'),\n",
       " (483, 'xe')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are gonna find what emoticons are used in our dataset\n",
    "import re\n",
    "tweets_text = train_data.SentimentText.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "sorted(emos_count,reverse=True)[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {':D', ':p', 'x)', ';-D', ';-)', ':-D', ';P', 'XD', 'xD', ':d', ';)', 'xd', ';d', ';p', ';D'}\n",
      "Sad emoticons: {':|', \":'(\", ':(', ':/'}\n"
     ]
    }
   ],
   "source": [
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Uncomment this line if you haven't downloaded punkt before\n",
    "# or just run it as it is and uncomment it if you got an error.\n",
    "#nltk.download('punkt')\n",
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 133899 different words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '!',\n",
       " '.',\n",
       " 'I',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'you',\n",
       " '?',\n",
       " 'a',\n",
       " 'it',\n",
       " 'i',\n",
       " '...',\n",
       " ';',\n",
       " 'and',\n",
       " '&',\n",
       " 'my',\n",
       " 'for',\n",
       " 'is',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'in',\n",
       " 'of',\n",
       " 'me']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_used_words(train_data.SentimentText.str.cat())[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 133899 different words\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "mw = most_used_words(train_data.SentimentText.str.cat())\n",
    "most_words = []\n",
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "    if w in stopwords.words(\"english\"):\n",
    "        continue\n",
    "    else:\n",
    "        most_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '!',\n",
       " '.',\n",
       " 'I',\n",
       " ',',\n",
       " '?',\n",
       " '...',\n",
       " ';',\n",
       " '&',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'quot',\n",
       " \"'m\",\n",
       " ':',\n",
       " '#',\n",
       " 'like',\n",
       " '-',\n",
       " 'get',\n",
       " 'good',\n",
       " 'u',\n",
       " 'know',\n",
       " ')',\n",
       " 'love',\n",
       " '(',\n",
       " 'one']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_words[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm defining this function to use it in the \n",
    "# Data Preparation Phase\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do some preprocessing of the tweets.\n",
    "# We will delete useless strings (like @, # ...)\n",
    "# because we think that they will not help\n",
    "# in determining if the person is Happy/Sad\n",
    "\n",
    "class TextPreProc(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, use_mention=False):\n",
    "        self.use_mention = use_mention\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We can choose between keeping the mentions\n",
    "        # or deleting them\n",
    "        if self.use_mention:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "            \n",
    "        # Keeping only the word after the #\n",
    "        X = X.str.replace(\"#\", \"\")\n",
    "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        # Removing HTML garbage\n",
    "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "        # Removing links\n",
    "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "        # replace repeated letters with only two occurences\n",
    "        # heeeelllloooo => heelloo\n",
    "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "        # mark emoticons as happy or sad\n",
    "        X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
    "        X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
    "        X = X.str.lower()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pipeline that will transform our tweets to something eatable.\n",
    "# You can see that we are using our previously defined stemmer, it will\n",
    "# take care of the stemming process.\n",
    "# For stop words, we let the inverse document frequency do the job\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentiments = train_data['Sentiment']\n",
    "tweets = train_data['SentimentText']\n",
    "\n",
    "# I get those parameters from the 'Fine tune the model' part\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc(use_mention=True)),\n",
    "    ('vectorizer', vectorizer),\n",
    "])\n",
    "\n",
    "# Let's split our data into learning set and testing set\n",
    "# This process is done to test the efficency of our model at the end.\n",
    "# You shouldn't look at the test data only after choosing the final model\n",
    "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)\n",
    "\n",
    "# This will tranform our learning data from simple text to vector\n",
    "# by going through the preprocessing tranformer.\n",
    "learning_data = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/black/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== logitic regression ===\n",
      "scores =  [0.81203189 0.81031756 0.8144938  0.81841247 0.80708897 0.81094767\n",
      " 0.80923451 0.81672103 0.80807348 0.81284848]\n",
      "mean =  0.8120169871871848\n",
      "variance =  1.2146670158113766e-05\n",
      "score on the learning data (accuracy) =  0.8717281975082867\n",
      "\n",
      "=== bernoulliNB ===\n",
      "scores =  [0.7898764  0.77943071 0.79187106 0.78949212 0.78558937 0.78451493\n",
      " 0.78253707 0.78888759 0.79379262 0.79175934]\n",
      "mean =  0.7877751186883548\n",
      "variance =  1.8993468686875977e-05\n",
      "score on the learning data (accuracy) =  0.9033175220025146\n",
      "\n",
      "=== multinomialNB ===\n",
      "scores =  [0.81170059 0.80886303 0.80847288 0.81018826 0.80185656 0.80566395\n",
      " 0.80834915 0.81199153 0.80414439 0.81140548]\n",
      "mean =  0.8082635813201623\n",
      "variance =  1.0447278063494593e-05\n",
      "score on the learning data (accuracy) =  0.8983740998971311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "lr = LogisticRegression()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "models = {\n",
    "    'logistic regression': lr,\n",
    "    'bernoulliNB': bnb,\n",
    "    'multinomialNB': mnb,\n",
    "}\n",
    "\n",
    "for model in models.keys():\n",
    "    scores = cross_val_score(models[model], learning_data, sentiments_learning, scoring=\"f1\", cv=10)\n",
    "    print(\"===\", model, \"===\")\n",
    "    print(\"scores = \", scores)\n",
    "    print(\"mean = \", scores.mean())\n",
    "    print(\"variance = \", scores.var())\n",
    "    models[model].fit(learning_data, sentiments_learning)\n",
    "    print(\"score on the learning data (accuracy) = \", accuracy_score(models[model].predict(learning_data), sentiments_learning))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_pre_processing__use_mention': True, 'vectorizer__max_features': None, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search_pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB()),\n",
    "])\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'text_pre_processing__use_mention': [True, False],\n",
    "        'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n",
    "        'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    },\n",
    "]\n",
    "grid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\n",
    "grid_search.fit(learn_data, sentiments_learning)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('text_pre_processing',\n",
       "                                        TextPreProc(use_mention=False)),\n",
       "                                       ('vectorizer',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range...\n",
       "                                       ('model',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid=[{'text_pre_processing__use_mention': [True, False],\n",
       "                          'vectorizer__max_features': [1000, 2000, 5000, 10000,\n",
       "                                                       20000, None],\n",
       "                          'vectorizer__ngram_range': [(1, 1), (1, 2)]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(learning_data, sentiments_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7551755175517552"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = pipeline.transform(test_data)\n",
    "mnb.score(testing_data, sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ItemID  Sentiment\n",
      "0            1          0\n",
      "1            2          0\n",
      "2            3          1\n",
      "3            4          0\n",
      "4            5          0\n",
      "...        ...        ...\n",
      "299984  299996          1\n",
      "299985  299997          1\n",
      "299986  299998          1\n",
      "299987  299999          1\n",
      "299988  300000          1\n",
      "\n",
      "[299989 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predecting on the test.csv\n",
    "sub_data = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "sub_learning = pipeline.transform(sub_data.SentimentText)\n",
    "sub = pd.DataFrame(sub_data.ItemID, columns=(\"ItemID\", \"Sentiment\"))\n",
    "sub[\"Sentiment\"] = mnb.predict(sub_learning)\n",
    "print(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so glad to meet you\n",
      "The probability that this tweet is sad is: 0.059035898277383286\n",
      "The probability that this tweet is happy is: 0.940964101722616\n"
     ]
    }
   ],
   "source": [
    "# Just run it\n",
    "model = MultinomialNB()\n",
    "model.fit(learning_data, sentiments_learning)\n",
    "tweet = pd.Series([input(),])\n",
    "tweet = pipeline.transform(tweet)\n",
    "proba = model.predict_proba(tweet)[0]\n",
    "print(\"The probability that this tweet is sad is:\", proba[0])\n",
    "print(\"The probability that this tweet is happy is:\", proba[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
